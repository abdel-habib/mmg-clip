{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AveragedMedicalCLIPLoss implementation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement: When working with medical text reports that consists of multiple labels or sentences. Some of the text might have similar meanings where it could refer to one or more images. Similarly, one image could be expressed with one or more report sections (from the same or different reports) or even labels. CLIP loss doesn't take into accound the underlying similarity of the text tokenized, thus for a large batch size (could consist of similar medical labels / sentences), the loss computation will be highly impacted as labels/texts are repeated in the formed logits [n,n] matrix. This mainly arrises when labels are similar, making the dot product assign high logits to several labels. This problem restricts the usage of CLIP in binary or multi-class for a list of text or labels, as they will be repeated on each column for the logits per image.\n",
    "\n",
    "One way to handle it is to make the batch size equal to the unique number of classes, thus for binary task, a batch size of 2 is used, while for multi-class, a batch size of n_classes is used. This affects the training as the batch size is very low, assuming that the dataloaded are equally sampelled. \n",
    "\n",
    "To overcome this, we introduce AveragedMedicalCLIPLoss. A loss function that takes into account the similarity between the labels/text for each batch independently using the cosine similarity of the text embeddings. This way, texts with a similarity that satisfies a threshold are given the same label, where unique texts are given unique labels.\n",
    "\n",
    "This way, we are able to average the logits per image for identical text, and compute the clip loss for each of the logits type as per the official implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import mmgclip\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "# for auto reload when changes are made in the package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m\t | config[epochs] will change from '15' to '30'\n",
      "\u001b[32mINFO\u001b[0m\t | config[annotated_dataset_path] will change from '/local/abdel/mmg-clip/mmgclip/../data/02_data_T_regions' to '../data/02_data_T_regions'\n",
      "\u001b[32mINFO\u001b[0m\t | config[experiment_name] will change from 'clip' to 'ClassifierExperiment'\n",
      "\u001b[32mINFO\u001b[0m\t | config[export_dir] will change from 'outputs' to '../outputs'\n",
      "\u001b[32mINFO\u001b[0m\t | config[warmup_epochs] will change from '0.1' to '0.1'\n",
      "\u001b[32mINFO\u001b[0m\t | config[weight_decay] will change from '0.0001' to '0.0001'\n",
      "\u001b[32mINFO\u001b[0m\t | config[train_batch_size] will change from '128' to '128'\n",
      "\u001b[32mINFO\u001b[0m\t | config[val_test_batch_size] will change from '128' to '128'\n",
      "\u001b[32mINFO\u001b[0m\t | config[loss_type] will change from 'CLIPLoss' to 'CLIPLoss'\n",
      "\u001b[32mINFO\u001b[0m\t | config[projection_name] will change from 'LinearProjectionLayer' to 'MultiLinearHead'\n",
      "\u001b[32mINFO\u001b[0m\t | config[learning_rate] will change from '5e-05' to '5e-05'\n",
      "\u001b[32mINFO\u001b[0m\t | config[run_name] will change from 'run01' to 'CLIPLoss_n=128'\n",
      "\u001b[32mINFO\u001b[0m\t | config[output_projection_dimension] will change from '512' to '256'\n"
     ]
    }
   ],
   "source": [
    "mmgconfig = mmgclip.config()\n",
    "mmgconfig.epochs = 30\n",
    "mmgconfig.annotated_dataset_path = '../data/02_data_T_regions'\n",
    "\n",
    "# create an experiment name, all outputs will be stored inside\n",
    "# default output dir is 'outputs', experiment output path will be \n",
    "# {mmgconfig.export_dir}/{mmgconfig.experiment_name}\n",
    "mmgconfig.experiment_name = 'ClassifierExperiment'\n",
    "mmgconfig.export_dir = '../outputs' \n",
    "mmgconfig.warmup_epochs = 0.1\n",
    "mmgconfig.weight_decay = 1e-4\n",
    "mmgconfig.train_batch_size = 128\n",
    "mmgconfig.val_test_batch_size = 128\n",
    "mmgconfig.loss_type = 'CLIPLoss'\n",
    "mmgconfig.projection_name = 'MultiLinearHead'\n",
    "mmgconfig.learning_rate = 5e-5\n",
    "mmgconfig.run_name = f'{mmgconfig.loss_type}_n={mmgconfig.train_batch_size}'\n",
    "mmgconfig.output_projection_dimension = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a checkpoint for experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m\t | Initializing pretrained `emilyalsentzer/Bio_ClinicalBERT` as the text encoder and tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m\t | Embeddings are projected to 256 features using MultiLinearHead.\n"
     ]
    }
   ],
   "source": [
    "model = mmgclip.model(config=mmgconfig)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        os.path.join(mmgconfig.export_dir, mmgconfig.experiment_name, mmgconfig.run_name, 'checkpoints', '20240306_6f4bbca6_lossCLIPLoss_epo30_seed42_lr5e-05_weight_decay0.001warmup_epochs0.1_train_bs128_projectionMultiLinearHead256.pth'))['model_state_dict'])\n",
    "\n",
    "clf   = mmgclip.PromptClassifier(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the labels has to be equal to the n batch size of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "# tokenize the labels using the same model tokenizer\n",
    "labels = [\n",
    "    'this benign',\n",
    "    'malignant',\n",
    "    'benign case',\n",
    "    'malignant case',\n",
    "    'benign',\n",
    "    'malignant',\n",
    "    'benign',\n",
    "    'xyz'\n",
    "    ]\n",
    "\n",
    "# labels = [\n",
    "#     'circumscribed',\n",
    "#     'obsecured',\n",
    "#     'microlobulated',\n",
    "#     'indistinct',\n",
    "#     'spiculated',\n",
    "#     'microlobulated',\n",
    "#     'indistinct',\n",
    "#     'spiculated',\n",
    "#     ]\n",
    "\n",
    "text_tokens = clf.tokenizer(\n",
    "    labels, \n",
    "    padding=\"max_length\", \n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=model.config.sequence_length)\n",
    "\n",
    "print(text_tokens['input_ids'].shape)\n",
    "\n",
    "batch = dict()\n",
    "batch['text_tokens'] = text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the text embeddings. Note that here we must pass through the learned projextion layer other wise the embeddings will be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings = model.encode_text(batch=batch)\n",
    "text_embeddings = model.text_projection_layer(text_embeddings)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True) \n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the cosine similarity between each and every sentence combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.7714, 0.8803, 0.8041, 0.9058, 0.7714, 0.9058, 0.7866],\n",
      "        [0.7714, 1.0000, 0.8387, 0.9547, 0.8594, 1.0000, 0.8594, 0.6772],\n",
      "        [0.8803, 0.8387, 1.0000, 0.9082, 0.9064, 0.8387, 0.9064, 0.8011],\n",
      "        [0.8041, 0.9547, 0.9082, 1.0000, 0.8636, 0.9547, 0.8636, 0.7153],\n",
      "        [0.9058, 0.8594, 0.9064, 0.8636, 1.0000, 0.8594, 1.0000, 0.7719],\n",
      "        [0.7714, 1.0000, 0.8387, 0.9547, 0.8594, 1.0000, 0.8594, 0.6772],\n",
      "        [0.9058, 0.8594, 0.9064, 0.8636, 1.0000, 0.8594, 1.0000, 0.7719],\n",
      "        [0.7866, 0.6772, 0.8011, 0.7153, 0.7719, 0.6772, 0.7719, 1.0000]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'benign vs. benign': 1.0000001192092896,\n",
       " 'malignant vs. malignant': 1.0,\n",
       " 'malignant vs. malignant case': 0.9547221660614014,\n",
       " 'malignant case vs. malignant': 0.9547221660614014,\n",
       " 'benign case vs. malignant case': 0.9082435965538025,\n",
       " 'benign case vs. benign': 0.9063839912414551,\n",
       " 'this benign vs. benign': 0.9057816863059998,\n",
       " 'this benign vs. benign case': 0.8802900910377502,\n",
       " 'malignant case vs. benign': 0.8636056184768677,\n",
       " 'malignant vs. benign': 0.8594213724136353,\n",
       " 'benign vs. malignant': 0.8594213724136353,\n",
       " 'malignant vs. benign case': 0.8387349247932434,\n",
       " 'benign case vs. malignant': 0.8387349247932434,\n",
       " 'this benign vs. malignant case': 0.8040817379951477,\n",
       " 'benign case vs. xyz': 0.8010841012001038,\n",
       " 'this benign vs. xyz': 0.786582887172699,\n",
       " 'benign vs. xyz': 0.7719182968139648,\n",
       " 'this benign vs. malignant': 0.771402895450592,\n",
       " 'malignant case vs. xyz': 0.7153266072273254,\n",
       " 'malignant vs. xyz': 0.677194356918335}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mesaure_embeddings_similarity(embeddings):\n",
    "    return util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "def embedding_labels_similarities(similarities, labels):\n",
    "    d = {}\n",
    "    for i, v1 in enumerate(labels):\n",
    "        for j, v2 in enumerate(labels):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            d[v1 + ' vs. ' + v2] = similarities[i][j].item()\n",
    "\n",
    "    # sort by score\n",
    "    d_sorted = dict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "    return d_sorted\n",
    "\n",
    "# measure the cosine similarity\n",
    "cosine_scores = mesaure_embeddings_similarity(text_embeddings)\n",
    "print(cosine_scores)\n",
    "# for visualizing the scores, print them \n",
    "d_sorted = embedding_labels_similarities(cosine_scores, labels)\n",
    "d_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a unique label to each of the similar values higher than the speicified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "def assign_labels(cosine_sim_matrix, threshold=0.7):\n",
    "    num_texts = cosine_sim_matrix.shape[0]\n",
    "    labels = [-1] * num_texts  # Initialize labels with -1\n",
    "    \n",
    "    current_label = 0\n",
    "    \n",
    "    for i in range(num_texts):\n",
    "        if labels[i] == -1:  # If the text hasn't been assigned a label yet\n",
    "            labels[i] = current_label  # Assign it the current label\n",
    "            for j in range(i+1, num_texts):\n",
    "                if cosine_sim_matrix[i][j] >= threshold:\n",
    "                    if labels[j] == -1:\n",
    "                        labels[j] = current_label  # Assign the same label if similarity >= threshold\n",
    "            current_label += 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "list_labels = assign_labels(cosine_scores, threshold=0.8)\n",
    "print(list_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss implementation based on text similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_image = torch.tensor(\n",
    "    [[-0.3695, -0.8987, -0.3323, -0.3540, -0.3375, -0.5998, -0.3583, -0.0797],\n",
    "     [-0.9398, -1.1682, -0.9602, -0.7505, -1.0275, -0.5558, -0.3456, -0.3068],\n",
    "     [-0.8346, -1.1233, -0.7055, -0.4546, -0.6598, -0.6412, -0.6927, -0.1958],\n",
    "     [-0.8875, -1.3657, -0.6414, -0.8099, -0.8178, -0.8100, -0.6184, -0.1464],\n",
    "     [-0.7839, -1.2652, -0.6129, -0.4527, -0.5410, -0.4618, -0.4844, -0.3835],\n",
    "     [-1.0263, -1.3110, -0.7902, -0.7323, -0.6832, -0.9224, -0.6688, -0.6417],\n",
    "     [-0.5663, -0.5041, -0.5145, -0.0413, -0.2905, -0.2322, -0.3936,  0.0914],\n",
    "     [-0.1942, -0.7119, -0.3226, -0.1033, -0.2929, -0.1779, -0.2586, -0.1330]])\n",
    "\n",
    "logits_per_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3354, 0.2250, 0.4396],\n",
      "        [0.2786, 0.2631, 0.4583],\n",
      "        [0.2929, 0.2368, 0.4703],\n",
      "        [0.2813, 0.2017, 0.5170],\n",
      "        [0.3378, 0.2531, 0.4091],\n",
      "        [0.3493, 0.2495, 0.4012],\n",
      "        [0.2805, 0.2785, 0.4410],\n",
      "        [0.3428, 0.2777, 0.3794]])\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(list_labels)\n",
    "# num_unique_labels = len(unique_labels)\n",
    "averaged_logits = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    label_indices = [i for i, l in enumerate(list_labels) if l == label]\n",
    "    label_logits = logits_per_image[:, label_indices]\n",
    "    averaged_logit = torch.mean(label_logits, dim=1)\n",
    "    averaged_logits.append(averaged_logit)\n",
    "\n",
    "averaged_logits_tensor = torch.stack(averaged_logits, dim=1)\n",
    "print(averaged_logits_tensor.softmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0, 0, 1, 0, 2])\n",
      "tensor(1.2048)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "labels = torch.tensor(list_labels)\n",
    "print(labels)\n",
    "\n",
    "loss_i = F.cross_entropy(averaged_logits_tensor, labels)\n",
    "\n",
    "print(loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
